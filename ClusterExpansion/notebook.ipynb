{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9d5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../Netket/')\n",
    "import netket as nk\n",
    "from jax import numpy as jnp\n",
    "import itertools\n",
    "from scipy.special import comb\n",
    "from jax import jit, vmap\n",
    "import jax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4984f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "hilbert = nk.hilbert.Spin(s=0.5, N=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee5ad51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hstates = hilbert.all_states()\n",
    "hilbert.states_to_numbers(hstates)\n",
    "# hstates[0], hilbert.states_to_numbers(hstates[0])\n",
    "# type(hilbert.states_to_numbers(hstates[0]))\n",
    "\n",
    "# type(jnp.max(np.array([2,3])))\n",
    "# print(len(hstates))\n",
    "# hilbert.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae28256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_parity(bitstring):\n",
    "    '''\n",
    "    bitstring is a cluster, ex s1s2, or s1s2s3s4 etc\n",
    "    type of bitstring: jnp.array, dtype=jnp.int8\n",
    "    returns +1 if even parity, -1 if odd parity\n",
    "    '''\n",
    "    par = jnp.prod(bitstring,axis=-1,dtype=jnp.int8) #Since these are spin states +1 and -1\n",
    "    return par \n",
    "\n",
    "def naive_cluster_expansion_mat(hilbert):\n",
    "    '''Compute cluster expansion coefficients up to a given maximum cluster size.\n",
    "    for now only works for spin-1/2 systems\n",
    "    '''\n",
    "    # n_sites = hilbert.n_sites\n",
    "    n_sites = hilbert.size\n",
    "    hstates = hilbert.all_states()\n",
    "    matsize = 2**n_sites\n",
    "    mat = jnp.ones((matsize, matsize),dtype=jnp.int8)\n",
    "    for state_idx, state in enumerate(hstates):\n",
    "        start_idx = 1 #First column is all ones, so start from second column\n",
    "        for cluster_size in jnp.arange(1, n_sites + 1):\n",
    "            clusters = jnp.array(list(itertools.combinations(state, cluster_size)))\n",
    "            rowvals = return_parity(clusters)\n",
    "            mat = mat.at[state_idx, start_idx: start_idx + int(comb(n_sites, cluster_size))].set(rowvals)\n",
    "            start_idx += int(comb(int(n_sites), cluster_size))\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637a0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_cluster_expansion(hilbert):\n",
    "    '''Maximum performance cluster expansion for large systems (N=16).\n",
    "    Uses vmap for automatic vectorization and JIT compilation.\n",
    "    '''\n",
    "    n_sites = hilbert.size\n",
    "    hstates = hilbert.all_states()\n",
    "    matsize = 2**n_sites\n",
    "    \n",
    "    # Pre-compute all cluster indices outside JIT (Python operations)\n",
    "    cluster_indices_list = []\n",
    "    for cluster_size in range(1, n_sites + 1):\n",
    "        clusters = list(itertools.combinations(range(n_sites), cluster_size))\n",
    "        cluster_indices_list.append(jnp.array(clusters, dtype=jnp.int32))\n",
    "    \n",
    "    total_clusters = sum(len(c) for c in cluster_indices_list)\n",
    "    \n",
    "    # Vectorized parity computation\n",
    "    @jit\n",
    "    def compute_parities_vectorized(state, clusters):\n",
    "        \"\"\"Compute parities for a single state across all clusters of a given size\"\"\"\n",
    "        # Shape: (n_clusters, cluster_size)\n",
    "        state_values = state[clusters]\n",
    "        # Compute parity: product along cluster axis\n",
    "        return jnp.prod(state_values, axis=-1, dtype=jnp.int8)\n",
    "    \n",
    "    # vmap over all states for each cluster size\n",
    "    @jit\n",
    "    def compute_all_parities(hstates, clusters):\n",
    "        \"\"\"Vectorized over all states\"\"\"\n",
    "        return vmap(lambda state: compute_parities_vectorized(state, clusters))(hstates)\n",
    "    \n",
    "    # Build matrix: move Python loop outside JIT\n",
    "    columns = [jnp.ones((matsize, 1), dtype=jnp.int8)]\n",
    "    \n",
    "    for clusters in cluster_indices_list:\n",
    "        parities = compute_all_parities(hstates, clusters)\n",
    "        columns.append(parities)\n",
    "    \n",
    "    # Concatenate all columns\n",
    "    mat = jnp.concatenate(columns, axis=1)\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e65b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1,  1,  1,  1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [ 1, -1,  1, -1],\n",
       "       [ 1, -1, -1,  1]], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_cluster_expansion_mat(nk.hilbert.Spin(s=0.5, N=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b49dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_cluster_expansion_extreme(hilbert):\n",
    "    '''Extreme optimization: just use the vmap version but with proper JIT wrapping.\n",
    "    The \"naive\" nested approach was causing issues with variable-sized clusters.\n",
    "    This sticks with the working vmap pattern but adds extra optimization.\n",
    "    '''\n",
    "    n_sites = hilbert.size\n",
    "    hstates = hilbert.all_states()\n",
    "    matsize = 2**n_sites\n",
    "    \n",
    "    # Pre-compute all cluster indices outside JIT (Python operations)\n",
    "    cluster_indices_list = []\n",
    "    for cluster_size in range(1, n_sites + 1):\n",
    "        clusters = list(itertools.combinations(range(n_sites), cluster_size))\n",
    "        cluster_indices_list.append(jnp.array(clusters, dtype=jnp.int32))\n",
    "    \n",
    "    # Double vmap: over all clusters at once per state\n",
    "    @jit\n",
    "    def compute_all_parities_for_state(state, all_cluster_arrays):\n",
    "        \"\"\"Compute all parities for a single state across all cluster sizes\"\"\"\n",
    "        # Process each cluster size\n",
    "        results = []\n",
    "        for clusters in all_cluster_arrays:\n",
    "            state_values = state[clusters]\n",
    "            parities = jnp.prod(state_values, axis=-1, dtype=jnp.int8)\n",
    "            results.append(parities)\n",
    "        return jnp.concatenate(results)\n",
    "    \n",
    "    # vmap over all states\n",
    "    @jit\n",
    "    def compute_all_states(hstates, all_cluster_arrays):\n",
    "        return vmap(lambda state: compute_all_parities_for_state(state, all_cluster_arrays))(hstates)\n",
    "    \n",
    "    # Compute all parities at once\n",
    "    parities = compute_all_states(hstates, cluster_indices_list)\n",
    "    \n",
    "    # Prepend identity column\n",
    "    identity_col = jnp.ones((matsize, 1), dtype=jnp.int8)\n",
    "    mat = jnp.concatenate([identity_col, parities], axis=1)\n",
    "    \n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec28cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1,  1,  1, ...,  1,  1,  1],\n",
       "       [ 1,  1,  1, ..., -1, -1, -1],\n",
       "       [ 1,  1,  1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [ 1, -1, -1, ...,  1,  1, -1],\n",
       "       [ 1, -1, -1, ...,  1,  1, -1],\n",
       "       [ 1, -1, -1, ..., -1, -1,  1]], dtype=int8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_cluster_expansion_extreme(nk.hilbert.Spin(s=0.5, N=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0726e04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1,  1,  1, ...,  1,  1,  1],\n",
       "       [ 1,  1,  1, ..., -1, -1, -1],\n",
       "       [ 1,  1,  1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [ 1, -1, -1, ...,  1,  1, -1],\n",
       "       [ 1, -1, -1, ...,  1,  1, -1],\n",
       "       [ 1, -1, -1, ..., -1, -1,  1]], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_cluster_expansion(nk.hilbert.Spin(s=0.5, N=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dbb74",
   "metadata": {},
   "source": [
    "### Tests for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68e37b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests for correctness: compare naive vs extreme optimized implementations\n",
    "spin_size = 6\n",
    "hil = nk.hilbert.Spin(s=0.5, N=spin_size)\n",
    "\n",
    "res_naive = naive_cluster_expansion_mat(hil)\n",
    "res_extreme = optim_cluster_expansion_extreme(hil)\n",
    "res_vec = optim_cluster_expansion(hil)\n",
    "\n",
    "np.isclose(res_naive, res_extreme).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64009a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Benchmarking N=10 (matrix size: 1024 x ~1024.0)\n",
      "============================================================\n",
      "\n",
      "JAX/GPU warmup complete\n",
      "\n",
      "[1] optim_cluster_expansion (vmap + JIT):\n",
      "    Time: 0.706s\n",
      "    Output shape: (1024, 1024)\n",
      "\n",
      "[2] optim_cluster_expansion_extreme (full vmap + JIT):\n",
      "    Time: 0.157s\n",
      "    Output shape: (1024, 1024)\n",
      "\n",
      "Results match: True\n",
      "\n",
      "============================================================\n",
      "Speedup: 4.49x\n",
      "============================================================\n",
      "\n",
      "    Time: 0.706s\n",
      "    Output shape: (1024, 1024)\n",
      "\n",
      "[2] optim_cluster_expansion_extreme (full vmap + JIT):\n",
      "    Time: 0.157s\n",
      "    Output shape: (1024, 1024)\n",
      "\n",
      "Results match: True\n",
      "\n",
      "============================================================\n",
      "Speedup: 4.49x\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_cluster_expansion(n_sites=8):\n",
    "    \"\"\"Benchmark all three implementations\"\"\"\n",
    "    hilbert = nk.hilbert.Spin(s=0.5, N=n_sites)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmarking N={n_sites} (matrix size: {2**n_sites} x ~{sum(comb(n_sites, k) for k in range(1, n_sites+1)) + 1})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Warmup\n",
    "    try:\n",
    "        optim_cluster_expansion_extreme(nk.hilbert.Spin(s=0.5, N=4))\n",
    "        print(\"JAX/GPU warmup complete\")\n",
    "    except:\n",
    "        print(\"GPU not available, using CPU\")\n",
    "    \n",
    "    # Test 1: Optimized with vmap\n",
    "    print(\"\\n[1] optim_cluster_expansion (vmap + JIT):\")\n",
    "    start = time.time()\n",
    "    result_vmap = optim_cluster_expansion(hilbert)\n",
    "    elapsed_vmap = time.time() - start\n",
    "    print(f\"    Time: {elapsed_vmap:.3f}s\")\n",
    "    print(f\"    Output shape: {result_vmap.shape}\")\n",
    "    \n",
    "    # Test 2: Extreme optimization\n",
    "    print(\"\\n[2] optim_cluster_expansion_extreme (full vmap + JIT):\")\n",
    "    start = time.time()\n",
    "    result_extreme = optim_cluster_expansion_extreme(hilbert)\n",
    "    elapsed_extreme = time.time() - start\n",
    "    print(f\"    Time: {elapsed_extreme:.3f}s\")\n",
    "    print(f\"    Output shape: {result_extreme.shape}\")\n",
    "    \n",
    "    # Verify correctness\n",
    "    print(f\"\\nResults match: {jnp.allclose(result_vmap, result_extreme)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Speedup: {elapsed_vmap/elapsed_extreme:.2f}x\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return result_extreme\n",
    "\n",
    "# Run benchmark for N=16\n",
    "mat_n16 = benchmark_cluster_expansion(n_sites=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84f98515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1,  1,  1,  1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [ 1, -1,  1, -1],\n",
       "       [ 1, -1, -1,  1]], dtype=int8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_cluster_expansion_extreme(nk.hilbert.Spin(s=0.5, N=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "395bf323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1, 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,\n",
       "       1. , 1. , 1. ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_test = 0.1 * np.ones(16)\n",
    "cluster_mat = optim_cluster_expansion_extreme(nk.hilbert.Spin(s=0.5, N=4)) \n",
    "cluster_coeffs = jnp.linalg.solve(cluster_mat, np.log(psi_test))\n",
    "print(1./np.linalg.norm(psi_test))\n",
    "np.exp(cluster_coeffs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fc5b650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FWHT verification for N=4\n",
      "Max absolute reconstruction error: 3.51e-16\n",
      "FWHT verified (complex psi) for N=4\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "@jit\n",
    "def fwht(x):\n",
    "    \"\"\"In-place Fast Walshâ€“Hadamard Transform (returns transformed vector).\n",
    "    Supports complex inputs (uses complex128 internally).\n",
    "    Length of x must be a power of two.\n",
    "    \"\"\"\n",
    "    x = x.astype(jnp.complex128)\n",
    "    n = x.shape[0]\n",
    "    h = 1\n",
    "    while h < n:\n",
    "        x = x.reshape(-1, 2 * h)\n",
    "        left = x[:, :h]\n",
    "        right = x[:, h:2 * h]\n",
    "        x = jnp.concatenate([left + right, left - right], axis=1)\n",
    "        x = x.reshape(n)\n",
    "        h *= 2\n",
    "    return x\n",
    "\n",
    "# Helper that maps your hilbert ordering to FWHT ordering, runs FWHT and returns coeffs in cluster-matrix column order\n",
    "def fwht_coeffs_in_cluster_col_order(psi, hilbert):\n",
    "    \"\"\"Compute coefficients c solving H c = psi using FWHT.\n",
    "    Returns c in the same column ordering as optim_cluster_expansion_extreme produces.\n",
    "    \"\"\"\n",
    "    n_sites = hilbert.size\n",
    "    hstates = np.array(hilbert.all_states())  # shape (n_states, n_sites) with values +1/-1\n",
    "\n",
    "    # Map spins s in {+1,-1} -> bits b in {0,1} with convention b=1 when s==-1\n",
    "    b = ((1 - hstates) // 2).astype(np.int64)\n",
    "    powers = (1 << np.arange(n_sites)).astype(np.int64)\n",
    "    indices = (b * powers).sum(axis=1)\n",
    "    perm = np.argsort(indices)  # mapping FWHT index -> row\n",
    "\n",
    "    psi_arr = jnp.array(psi)\n",
    "    psi_by_index = psi_arr[perm]\n",
    "    n = psi_by_index.shape[0]\n",
    "    coeffs_by_index = fwht(psi_by_index) / float(n)  # indexed by subset mask\n",
    "    coeffs_by_index_np = np.array(coeffs_by_index)\n",
    "\n",
    "    # Build mask list in the SAME ORDER as optim_cluster_expansion_extreme columns\n",
    "    masks = [0]\n",
    "    for cluster_size in range(1, n_sites + 1):\n",
    "        for comb in itertools.combinations(range(n_sites), cluster_size):\n",
    "            mask = 0\n",
    "            for bpos in comb:\n",
    "                mask |= (1 << bpos)\n",
    "            masks.append(mask)\n",
    "    masks = np.array(masks, dtype=np.int64)  # length n\n",
    "\n",
    "    # coeffs in column order: take coeffs_by_index[mask] for each column\n",
    "    coeffs_col_order = coeffs_by_index_np[masks]\n",
    "    return coeffs_col_order\n",
    "\n",
    "# Verification test for N=4 with complex psi\n",
    "N = 4\n",
    "hil = nk.hilbert.Spin(s=0.5, N=N)\n",
    "print(f\"Running FWHT verification for N={N}\")\n",
    "\n",
    "# random complex psi in hilbert ordering\n",
    "psi = (np.random.rand(2**N) + 1j * np.random.rand(2**N)).astype(np.complex128)\n",
    "coeffs_col = fwht_coeffs_in_cluster_col_order(psi, hil)\n",
    "\n",
    "# Reconstruct psi using the explicit matrix (small N)\n",
    "cluster_mat = optim_cluster_expansion_extreme(hil)\n",
    "cluster_mat_c = jnp.array(cluster_mat, dtype=jnp.complex128)\n",
    "recon = cluster_mat_c @ jnp.array(coeffs_col)\n",
    "\n",
    "max_err = float(jnp.max(jnp.abs(recon - psi)))\n",
    "print(f\"Max absolute reconstruction error: {max_err:.2e}\")\n",
    "assert jnp.allclose(recon, jnp.array(psi), atol=1e-8), \"FWHT reconstruction failed\"\n",
    "print(\"FWHT verified (complex psi) for N=4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ae45b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1+0.j, 0. +0.j, 0. +0.j, ..., 0. +0.j, 0. +0.j, 0. +0.j],\n",
       "      shape=(65536,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sites = 16\n",
    "psi_test = 0.1 * np.ones(2**n_sites)\n",
    "fwht_coeffs_in_cluster_col_order(psi_test, nk.hilbert.Spin(0.5,n_sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aed0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
